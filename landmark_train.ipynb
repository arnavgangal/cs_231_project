{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78aea33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4eeceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e8c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, phase, grayscale_transform=None, color_transform=None):\n",
    "        self.grayscale_dir = os.path.join(root_dir, phase, 'grayscale')\n",
    "        self.color_dir = os.path.join(root_dir, phase, 'color')\n",
    "        self.grayscale_transform = grayscale_transform\n",
    "        self.color_transform = color_transform\n",
    "        self.image_names = [f for f in os.listdir(self.grayscale_dir) if os.path.isfile(os.path.join(self.grayscale_dir, f))]\n",
    "        \n",
    "        # Collect labels\n",
    "        self.labels = []\n",
    "        self.valid_image_names = []\n",
    "        for filename in self.image_names:\n",
    "            if filename.startswith('grayscale_') or filename.startswith('color_'):\n",
    "                parts = filename.split('_')\n",
    "                try:\n",
    "                    label = int(parts[1])\n",
    "                    self.labels.append(label)\n",
    "                    self.valid_image_names.append(filename)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        self.image_names = self.valid_image_names\n",
    "\n",
    "        # Create continuous label mapping\n",
    "        unique_labels = sorted(set(self.labels))\n",
    "        self.label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        self.labels = [self.label_mapping[label] for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        grayscale_path = os.path.join(self.grayscale_dir, self.image_names[idx])\n",
    "        color_path = os.path.join(self.color_dir, self.image_names[idx])\n",
    "\n",
    "        grayscale_image = Image.open(grayscale_path).convert('L')\n",
    "        color_image = Image.open(color_path).convert('RGB')\n",
    "\n",
    "        if self.grayscale_transform:\n",
    "            grayscale_image = self.grayscale_transform(grayscale_image)\n",
    "        if self.color_transform:\n",
    "            color_image = self.color_transform(color_image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return grayscale_image, color_image, label\n",
    "\n",
    "grayscale_transform = transforms.Compose([\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "color_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576c492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "root_dir = 'dataset5_dual_data'\n",
    "train_dataset = PairedImageDataset(root_dir, 'train', grayscale_transform, color_transform)\n",
    "val_dataset = PairedImageDataset(root_dir, 'val', grayscale_transform, color_transform)\n",
    "test_dataset = PairedImageDataset(root_dir, 'test', grayscale_transform, color_transform)\n",
    "\n",
    "# Create dataloader instances\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2a46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class DualInputCNN(nn.Module):\n",
    "    def __init__(self, num_classes=24):\n",
    "        super(DualInputCNN, self).__init__()\n",
    "\n",
    "        # Grayscale CNN\n",
    "        self.grayscale_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(512, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Landmark CNN\n",
    "        self.landmark_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(50),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(50, 25, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(25),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32*3*3 + 25*10*1, 512)\n",
    "        self.fc_dropout = nn.Dropout2d(0.3)\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Assuming 10 classes\n",
    "\n",
    "    def forward(self, grayscale, landmarks):\n",
    "        # Grayscale pathway\n",
    "        x1 = self.grayscale_cnn(grayscale)\n",
    "        x1 = x1.view(-1, 32*3*3)\n",
    "\n",
    "        # Color pathway\n",
    "        x2 = self.landmark_cnn(landmarks)\n",
    "        x2 = x2.view(-1, 25*10*1)\n",
    "\n",
    "        # Concatenate and classify\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc_dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1dfeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_mediapipe(color_images):\n",
    "    batch_landmarks = []\n",
    "    for color_image in color_images:\n",
    "        color_image_np = color_image.permute(1, 2, 0).cpu().numpy()  # Move to CPU before converting to numpy\n",
    "        color_image_np = (color_image_np * 255).astype(np.uint8)  # Convert to uint8\n",
    "        color_image_np = np.ascontiguousarray(color_image_np)  # Ensure contiguous array\n",
    "        results = mp_hands.process(color_image_np)\n",
    "        landmarks = []\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    landmarks.append([lm.x, lm.y, lm.z])\n",
    "        # If no landmarks found or fewer than 21, fill with zeros\n",
    "        while len(landmarks) < 21:\n",
    "            landmarks.append([0.0, 0.0, 0.0])\n",
    "        batch_landmarks.append(landmarks)\n",
    "    landmarks_tensor = torch.tensor(batch_landmarks, dtype=torch.float32)\n",
    "    return landmarks_tensor.view(len(color_images), 1, 21, 3).to(device)  # Move back to device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903c845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717269063.305104 3168953 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1717269063.315735 3169076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1717269063.320846 3169076 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Epoch 1/10 Training:   0%|                             | 0/1439 [00:00<?, ?it/s]/Users/arnavgangal/anaconda3/envs/cs224n/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "/Users/arnavgangal/anaconda3/envs/cs224n/lib/python3.12/site-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/10 Training: 100%|██████| 1439/1439 [19:34<00:00,  1.23it/s, loss=0.918]\n",
      "Epoch 1/10 Validation: 100%|██| 309/309 [03:40<00:00,  1.40it/s, val_loss=0.222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 0.918, Validation loss: 0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training: 100%|██████| 1439/1439 [19:25<00:00,  1.23it/s, loss=0.327]\n",
      "Epoch 2/10 Validation: 100%|█| 309/309 [03:38<00:00,  1.41it/s, val_loss=0.0994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training loss: 0.327, Validation loss: 0.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training: 100%|██████| 1439/1439 [19:26<00:00,  1.23it/s, loss=0.234]\n",
      "Epoch 3/10 Validation: 100%|█| 309/309 [03:38<00:00,  1.41it/s, val_loss=0.0674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training loss: 0.234, Validation loss: 0.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training:  26%|█▊     | 367/1439 [05:20<22:27,  1.26s/it, loss=0.187]"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = DualInputCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with accuracy calculation\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} Training')\n",
    "\n",
    "    for data in train_progress_bar:\n",
    "        grayscale_images, color_images, labels = data\n",
    "\n",
    "        # Move data to the device\n",
    "        grayscale_images = grayscale_images.to(device)\n",
    "        color_images = color_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Process color images to extract landmarks\n",
    "        landmarks = process_with_mediapipe(color_images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(grayscale_images, landmarks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        train_progress_bar.set_postfix({\n",
    "            'loss': running_loss / (train_progress_bar.n + 1),\n",
    "            'accuracy': correct_predictions / total_predictions\n",
    "        })\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    val_progress_bar = tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} Validation')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_progress_bar:\n",
    "            grayscale_images, color_images, labels = data\n",
    "\n",
    "            # Move data to the device\n",
    "            grayscale_images = grayscale_images.to(device)\n",
    "            color_images = color_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Process color images to extract landmarks\n",
    "            landmarks = process_with_mediapipe(color_images)\n",
    "\n",
    "            outputs = model(grayscale_images, landmarks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "            val_total_predictions += labels.size(0)\n",
    "\n",
    "            val_progress_bar.set_postfix({\n",
    "                'val_loss': val_loss / (val_progress_bar.n + 1),\n",
    "                'val_accuracy': val_correct_predictions / val_total_predictions\n",
    "            })\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Training loss: {running_loss / len(train_loader):.3f}, Training accuracy: {correct_predictions / total_predictions:.3f}, Validation loss: {val_loss / len(val_loader):.3f}, Validation accuracy: {val_correct_predictions / val_total_predictions:.3f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create batch of landmark processed images - full image annotation\n",
    "# def process_with_mediapipe_full(color_images):\n",
    "#     processed_images = []\n",
    "#     for color_image in color_images:\n",
    "#         color_image_np = color_image.permute(1, 2, 0).numpy()\n",
    "#         color_image_np = (color_image_np * 255).astype(np.uint8)  # Convert to uint8\n",
    "#         color_image_np = np.ascontiguousarray(color_image_np)  # Ensure contiguous array\n",
    "#         results = mp_hands.process(color_image_np)\n",
    "#         if results.multi_hand_landmarks:\n",
    "#             for hand_landmarks in results.multi_hand_landmarks:\n",
    "#                 mp.solutions.drawing_utils.draw_landmarks(\n",
    "#                     color_image_np, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "#         color_image_np = color_image_np.astype(np.float32) / 255.0  # Convert back to float32\n",
    "#         processed_images.append(torch.tensor(color_image_np).permute(2, 0, 1))\n",
    "#     return torch.stack(processed_images)\n",
    "\n",
    "\n",
    "\n",
    "# # Function to visualize a batch of images\n",
    "# def visualize_batch(grayscale_images, color_images, processed_color_images):\n",
    "#     batch_size = grayscale_images.size(0)\n",
    "\n",
    "#     fig, axes = plt.subplots(batch_size, 3, figsize=(12, batch_size * 4))\n",
    "#     for i in range(batch_size):\n",
    "#         # Grayscale image\n",
    "#         axes[i, 0].imshow(grayscale_images[i].permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "#         axes[i, 0].set_title('Grayscale Image')\n",
    "#         axes[i, 0].axis('off')\n",
    "\n",
    "#         # Original color image\n",
    "#         axes[i, 1].imshow(color_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "#         axes[i, 1].set_title('Original Color Image')\n",
    "#         axes[i, 1].axis('off')\n",
    "\n",
    "#         # Processed color image with MediaPipe landmarks\n",
    "#         axes[i, 2].imshow(processed_color_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "#         axes[i, 2].set_title('Processed Color Image')\n",
    "#         axes[i, 2].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Fetch a single batch of images from the DataLoader\n",
    "# data_iter = iter(train_loader)\n",
    "# grayscale_images, color_images, labels = next(data_iter)\n",
    "\n",
    "# # Process color images with MediaPipe\n",
    "# processed_color_images = process_with_mediapipe_full(color_images)\n",
    "\n",
    "# # Visualize the batch\n",
    "# visualize_batch(grayscale_images, color_images, processed_color_images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
